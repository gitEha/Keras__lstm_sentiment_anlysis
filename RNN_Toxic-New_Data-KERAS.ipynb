{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Ranet\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from process_data import process_document\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = json.load(open('toxic_data2.json', encoding='UTF8'))\n",
    "\n",
    "x_train = data['x_train']\n",
    "x_test = data['x_test']\n",
    "y_train = data['y_train']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9944 len of x train\n",
      "9944 len of y train\n",
      "2487 len of x test\n",
      "2487 len of y test\n",
      "Kohalikud venelased ehk saavadki kuidagi oma eraisiku tasemel ja nÃ¤gemusega hakkama aga kui riik oma kohaolu ignoreerib siis kohalikud hakkavad otsima neid kes neid EI ignereeri ja ulatab nii vajaliku abikÃ¤e.... Loodus tÃ¼hja kohta ei salli . Kui Eesti jaoks on Ida-Virumaa kui regioon pohh-nahh siis ehk siis silmad avanevad kui korraga juhtmed seinast vÃ¤ljatÃµmmatass? ja kus ã¼lejã¤ã¤nud eesti tã¶ã¶tajaskond on kes teenib miinimumi lã¤hedast palka ? istute mã¶kuna oma peldikus ja vahite ukseaugust pealt ? vã¤hemalt ametiã¼hingud peaksid kãµik nagu ã¼ks mees vã¤ljas olema aga nemad soojendavad kotte kuskil kontoora nurgas ja teevad valitsusega isiklikke diile .... aivo peterson 09 09 62.181.220.227\n",
      "105\n",
      "HÃ¤lvik on hÃ¤lvik.  tånis 13 09 194.150.65.37\n",
      "105\n",
      "Number of 100: 5358\n",
      "Number of 105: 4586\n",
      "Number of 100 in val: 1348\n",
      "Number of 105 in val: 1139\n",
      "Mean lengths of comments: 50.466311343523735\n",
      "Mean lengths of comments with the last 25 longest comments removed: 49.257586450247004\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train), 'len of x train')\n",
    "print(len(y_train), 'len of y train')\n",
    "\n",
    "print(len(x_test), 'len of x test')\n",
    "print(len(y_test), 'len of y test')\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "print(x_test[0])\n",
    "print(y_test[0])\n",
    "\n",
    "print(\"Number of 100: {}\".format(y_train.count(\"100\"))) #100 is improper\n",
    "print(\"Number of 105: {}\".format(y_train.count(\"105\")))\n",
    "\n",
    "print(\"Number of 100 in val: {}\".format(y_test.count(\"100\"))) # 100 is improper\n",
    "print(\"Number of 105 in val: {}\".format(y_test.count(\"105\")))\n",
    "\n",
    "X = x_train\n",
    "X_val = x_test\n",
    "\n",
    "y = [0 if int(y) == 100 else 1 for y in y_train]\n",
    "y_val = [0 if int(y) == 100 else 1 for y in y_test]\n",
    "\n",
    "\n",
    "from statistics import mean\n",
    "lengths = [len(x_ex.split(' ')) for x_ex in X]\n",
    "lengths.sort()\n",
    "print(\"Mean lengths of comments: {}\".format(mean(lengths)))\n",
    "print(\"Mean lengths of comments with the last 25 longest comments removed: {}\"\n",
    "      .format(mean(lengths[:-25])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HÃ¤lvik on hÃ¤lvik.  tå\\x8dnis 13 09 194.150.65.37', 'KAS TÃ•ESTI KÃ•IK OPOSITSIOON KAASAARVATUD EKRE JA KESKERAKOND ON NII LONTIS KÃ•RVADEGA JA KUULETUVAD JA LÃ„HEVAD KOALITSIOONIGA ÃœHTE PAATI,ET VALIDA. PAGULAS KVOOTE JA KOOSELUSEADUST SOOSIV KANDIDAAT. MIDA ARVAB RAHVAS,NENDE VALIJAD?  eestlane 22 09 90.191.230.90', '25 aastat(saatkonna rahadega-mÃµisa ehitamine)!\\nJÃ¤Ã¤b\\nAlles JÃ•KS! siimust ei rã¤ã¤gi me 75 aastat, seega jã¤ã¤b helme. helmest ei rã¤ã¤gi 14 09 90.191.198.152', \"Lp Ãµiguskantsler vÃµtab ette kriitika Vabariigi Valitsuse ja isegi Riigikogu aadressil ja eks ta peabki seda tegema, see on tema tÃ¶Ã¶. On meil olnud ennegi sÃµnakaid Ãµiguskantslerid, meenutage JÃµks'i. NÃ¤ib, et selle ametiga kÃ¤ib kaasas teatud kompleks olla targem kollektiivsest mÃµistusest, arvamus, et rahva valitud Riigikogu ei ole kompetentne, et Ãµiguskantsler on kompetentsem kui valitsus ja sajad riigiametnikest kÃµrgelt haritud spetsialistid kokku. Lisaks meie enda kollektiivsele mÃµistusele on meile suureks abiks meie suurte pikaajaliste kogemustega liitlased EL struktuuridest. Nii ei lÃ¤he, ei saa Ã¼ks riigiametnik seada kahtluse alla meie edumeelse riigi ja tema juhtide poolt valitud kiireid edusamme ja kompetentsust.  jaan ueson 06 09 80.235.63.182\", 'Kotletti ja vorsti Ã¼heaegselt!? Ei, tÃ¤nan! Ei ole minu maitsemeeltega kooskÃµlas.  emb kumb 13 09 193.40.56.86']\n",
      "[1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(X_val[0:5])\n",
    "print(y_val[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in vocab:  110133\n",
      "Kohalikud venelased ehk saavadki kuidagi oma eraisiku tasemel ja nÃ¤gemusega hakkama aga kui riik oma kohaolu ignoreerib siis kohalikud hakkavad otsima neid kes neid EI ignereeri ja ulatab nii vajaliku abikÃ¤e.... Loodus tÃ¼hja kohta ei salli . Kui Eesti jaoks on Ida-Virumaa kui regioon pohh-nahh siis ehk siis silmad avanevad kui korraga juhtmed seinast vÃ¤ljatÃµmmatass? ja kus ã¼lejã¤ã¤nud eesti tã¶ã¶tajaskond on kes teenib miinimumi lã¤hedast palka ? istute mã¶kuna oma peldikus ja vahite ukseaugust pealt ? vã¤hemalt ametiã¼hingud peaksid kãµik nagu ã¼ks mees vã¤ljas olema aga nemad soojendavad kotte kuskil kontoora nurgas ja teevad valitsusega isiklikke diile .... aivo peterson 09 09 62.181.220.227\n",
      "[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 6, 15, 16, 17, 18, 19, 20, 21, 22, 21, 23, 24, 9, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37]] Length: 40\n"
     ]
    }
   ],
   "source": [
    "class InputProcessor():\n",
    "    def __init__(self, X_data, vocab_size=30000, comment_length=40):        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.comment_length = comment_length\n",
    "        self.word_to_ix = {}\n",
    "        \n",
    "        #Initialize word_to_ix\n",
    "        self.add_word_to_ix(\"0\")\n",
    "        for comment in X_data:\n",
    "            for word in comment.split(' '):        \n",
    "                self.add_word_to_ix(word)\n",
    "        \n",
    "        \n",
    "    def add_word_to_ix(self, word):\n",
    "        if(word not in self.word_to_ix):\n",
    "            if len(self.word_to_ix) >= self.vocab_size:\n",
    "                self.word_to_ix[word] = self.vocab_size\n",
    "            else:\n",
    "                self.word_to_ix[word] = len(self.word_to_ix)\n",
    "                \n",
    "    def preprocess_input(self, sentences_to_process):\n",
    "        processed_sentences = []\n",
    "\n",
    "        for num, sentence in enumerate(sentences_to_process):\n",
    "            processed_sentences.append([])\n",
    "            sentence = sentence.split(' ')\n",
    "            # Crop sentence or add zero padding\n",
    "            if len(sentence) >= self.comment_length:\n",
    "                sentence = sentence[:self.comment_length]\n",
    "            else:\n",
    "                new_sentence = []\n",
    "                for i in range(self.comment_length):\n",
    "                    if i > len(sentence) - 1:\n",
    "                        new_sentence.append(\"0\")\n",
    "                    else:\n",
    "                        new_sentence.append(sentence[i])\n",
    "                sentence = new_sentence\n",
    "\n",
    "            for word in sentence:\n",
    "                if word in self.word_to_ix:\n",
    "                    processed_sentences[num].append(self.word_to_ix[word])\n",
    "                else:\n",
    "                    self.add_word_to_ix(word)\n",
    "                    processed_sentences[num].append(self.word_to_ix[word])\n",
    "                    \n",
    "        return processed_sentences\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_mini_batches(input_X, input_y, batch_size):\n",
    "        batched_X = []\n",
    "        batched_y = []\n",
    "        for i in range(int(len(input_X) / batch_size) + 1):\n",
    "            batched_X.append(input_X[i*batch_size:i*batch_size + batch_size])\n",
    "            batched_y.append(input_y[i*batch_size:i*batch_size + batch_size])\n",
    "            \n",
    "        return (batched_X, batched_y)\n",
    "        \n",
    "InputProc = InputProcessor(X)\n",
    "\n",
    "print('Total words in vocab: ', len(InputProc.word_to_ix))            \n",
    "test_x = InputProc.preprocess_input([X[0]])\n",
    "print(X[0])\n",
    "print(test_x, 'Length: ' + str(len(test_x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 35000\n",
    "embed_dim = 64\n",
    "lstm_out = 32\n",
    "\n",
    "def list_to_numpy(input_list):\n",
    "    inp = np.zeros((len(input_list), len(input_list[0])))\n",
    "    for num_ex, ex in enumerate(input_list):\n",
    "        for num_i, i in enumerate(ex):\n",
    "            inp[num_ex][num_i] = i\n",
    "    return inp\n",
    "        \n",
    "X = list_to_numpy(InputProc.preprocess_input(X))\n",
    "X_val = list_to_numpy(InputProc.preprocess_input(X_val))\n",
    "\n",
    "y_val = np.asarray(y_val)\n",
    "y = np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 0 0]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n",
      "[1 0 0 1 1]\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y[0:5])\n",
    "y = to_categorical(y, 2)\n",
    "print(y[0:5])\n",
    "print(y_val[0:5])\n",
    "y_val = to_categorical(y_val, 2)\n",
    "print(y_val[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Users\\Ranet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n",
      "  \n",
      "E:\\Users\\Ranet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(32, dropout=0.4, recurrent_dropout=0.4)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 64)            2240000   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 2,252,482\n",
      "Trainable params: 2,252,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embed_dim, input_length=X.shape[1], dropout=0.4))\n",
    "model.add(LSTM(lstm_out, dropout_U=0.4, dropout_W=0.4))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14.  6. 15. 16. 17.\n",
      " 18. 19. 20. 21. 22. 21. 23. 24.  9. 25. 26. 27. 28. 29. 30. 31. 32. 33.\n",
      " 34. 35. 36. 37.]\n",
      "[0. 1.]\n",
      "Train on 9944 samples, validate on 2487 samples\n",
      "Epoch 1/50\n",
      " - 18s - loss: 0.6743 - acc: 0.5764 - val_loss: 0.6121 - val_acc: 0.6590\n",
      "Epoch 2/50\n",
      " - 16s - loss: 0.5627 - acc: 0.7221 - val_loss: 0.5482 - val_acc: 0.7081\n",
      "Epoch 3/50\n",
      " - 16s - loss: 0.4358 - acc: 0.8075 - val_loss: 0.5589 - val_acc: 0.7189\n",
      "Epoch 4/50\n",
      " - 16s - loss: 0.3454 - acc: 0.8610 - val_loss: 0.5594 - val_acc: 0.7346\n",
      "Epoch 5/50\n",
      " - 16s - loss: 0.2787 - acc: 0.8964 - val_loss: 0.5695 - val_acc: 0.7330\n",
      "Epoch 6/50\n",
      " - 16s - loss: 0.2252 - acc: 0.9186 - val_loss: 0.6569 - val_acc: 0.7378\n",
      "Epoch 7/50\n",
      " - 16s - loss: 0.1894 - acc: 0.9333 - val_loss: 0.6751 - val_acc: 0.7346\n",
      "Epoch 8/50\n",
      " - 16s - loss: 0.1579 - acc: 0.9442 - val_loss: 0.7547 - val_acc: 0.7286\n",
      "Epoch 9/50\n",
      " - 16s - loss: 0.1327 - acc: 0.9558 - val_loss: 0.9129 - val_acc: 0.7230\n",
      "Epoch 10/50\n",
      " - 15s - loss: 0.1197 - acc: 0.9592 - val_loss: 0.8367 - val_acc: 0.7318\n",
      "Epoch 11/50\n",
      " - 19s - loss: 0.1058 - acc: 0.9634 - val_loss: 0.9693 - val_acc: 0.7230\n",
      "Epoch 12/50\n",
      " - 17s - loss: 0.0993 - acc: 0.9672 - val_loss: 0.8983 - val_acc: 0.7201\n",
      "Epoch 13/50\n",
      " - 16s - loss: 0.0858 - acc: 0.9704 - val_loss: 1.0573 - val_acc: 0.7197\n",
      "Epoch 14/50\n",
      " - 18s - loss: 0.0770 - acc: 0.9731 - val_loss: 1.0320 - val_acc: 0.7226\n",
      "Epoch 15/50\n",
      " - 16s - loss: 0.0664 - acc: 0.9757 - val_loss: 1.0500 - val_acc: 0.7189\n",
      "Epoch 16/50\n",
      " - 15s - loss: 0.0622 - acc: 0.9797 - val_loss: 1.2160 - val_acc: 0.7121\n",
      "Epoch 17/50\n",
      " - 16s - loss: 0.0563 - acc: 0.9800 - val_loss: 1.2564 - val_acc: 0.7129\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-68997fbf2159>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    966\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1667\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1668\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1669\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1671\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1204\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1206\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1207\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1208\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2473\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2474\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2475\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2476\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1128\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1129\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1342\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1344\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1345\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1348\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1351\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1329\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 32\n",
    "print(X[0])\n",
    "print(y[0])\n",
    "model.fit(X, y, epochs = NUM_EPOCHS, batch_size=BATCH_SIZE, verbose = 2, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'out' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-93b43559a502>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'out' is not defined"
     ]
    }
   ],
   "source": [
    "print(out[0][0])\n",
    "print(out.size())\n",
    "\n",
    "\n",
    "out = out.view(out.size()[1], out.size()[2])\n",
    "\n",
    "print(out.size())\n",
    "print(out[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'optim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bdbac8c54449>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'optim' is not defined"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(input_X, input_y, model, optimizer, criterion, InpPrep, epochs=5, batch_size=64):\n",
    "    \n",
    "    batched_X, batched_y = InputProc.create_mini_batches(input_X, input_y, batch_size)\n",
    "    \n",
    "    inputs_val = InpPrep.preprocess_input(X_val)\n",
    "    inp_val = autograd.Variable(torch.LongTensor(inputs_val))    \n",
    "\n",
    "    for epoch in range(epochs): \n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for batch_num, (x, y) in tqdm(enumerate(zip(batched_X, batched_y))):\n",
    "            model.zero_grad()\n",
    "            \n",
    "            x = InpPrep.preprocess_input(x)            \n",
    "            predicted = model(autograd.Variable(torch.LongTensor(x)))\n",
    "            predicted = predicted.view(predicted.size()[1], predicted.size()[2])\n",
    "                        \n",
    "            loss = criterion(predicted, autograd.Variable(torch.LongTensor(y)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            _, preds = torch.max(predicted.data, 1)\n",
    "            running_loss += loss.data[0]\n",
    "            running_corrects += torch.sum(preds == torch.LongTensor(y))\n",
    "            \n",
    "        \n",
    "        if epoch % 10 == 0:            \n",
    "            print('-' * 5)                                    \n",
    "            \n",
    "            # VALIDATION ACC\n",
    "            val_predictions = model(inp_val)\n",
    "            val_predictions = val_predictions.view(val_predictions.size()[1], val_predictions.size()[2])                \n",
    "            _, val_preds = torch.max(val_predictions.data, 1)        \n",
    "            val_summed = np.sum(val_preds.numpy() == y_val)\n",
    "                                \n",
    "            print('Traning accuracy: {:.2f}%'.format(running_corrects / len(input_y)))\n",
    "            print('Validation accuracy: {:.2f}%'.format(val_summed / len(val_preds)))\n",
    "            print(\"Epoch {} loss: {:f}, single: {:f}\".format(epoch, running_loss, running_loss / BATCH_SIZE))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-29e70bfe0fee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mInputProc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "model = train_model(X, y, model, optimizer, criterion, InputProc, epochs=50, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_preds = model(inp)\n",
    "training_preds = training_preds.view(training_preds.size()[1], training_preds.size()[2])\n",
    "\n",
    "print(training_preds.size())\n",
    "_, preds = torch.max(training_preds.data, 1)\n",
    "\n",
    "summed = np.sum(preds.numpy() == y)\n",
    "print('Traning accuracy: {}'.format(summed / len(training_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(X_val))\n",
    "inputs_val = InputProc.preprocess_input(X_val)\n",
    "print(X_val[0].split(' ')[:30])\n",
    "print(inputs_val[0])\n",
    "inp_val = autograd.Variable(torch.LongTensor(inputs_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_predictions = model(inp_val)\n",
    "val_predictions = val_predictions.view(val_predictions.size()[1], val_predictions.size()[2])\n",
    "print(val_predictions.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y_val[0:5])\n",
    "_, val_preds = torch.max(val_predictions.data, 1)\n",
    "print(val_preds[0])\n",
    "\n",
    "summed_val = np.sum(val_preds.numpy() == y_val)\n",
    "print('Validation accuracy: {}'.format(summed_val / len(val_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
